{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnaFattakhova/poetry-llm-additional/blob/main/syllable_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import json\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "qHtudnzcFzZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1 — стримим датасет\n",
        "# ссылка: https://huggingface.co/datasets/missvector/multi-wiki-grammar\n",
        "def streaming():\n",
        "    dataset = load_dataset(\"missvector/multi-wiki-grammar\")\n",
        "    df = dataset[\"train\"].to_pandas()\n",
        "     # Обрезаем до первых 1000 строк\n",
        "    df = df.head(1000)\n",
        "    return df"
      ],
      "metadata": {
        "id": "4GwWhmBLyPXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем для просмотра\n",
        "df = streaming()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "258zwJeN0xRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "GBjFi0MMSNgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Здесь есть проблема: Й заменяется на И\n",
        "# чистим, оставляем только слова на русском\n",
        "# [a-яА-Я]\n",
        "#def cleaning(text):\n",
        "#    if pd.isna(text):\n",
        "#        return text\n",
        "    # Одним regex удаляем всё, кроме русских букв и пробелов + чистим пробелы\n",
        "#    cleaned_text = re.sub(r'[^а-яА-ЯёЁ\\s]+|\\s+', ' ', str(text)).strip()\n",
        "#    return cleaned_text if cleaned_text else pd.NA"
      ],
      "metadata": {
        "id": "_KVVvlRS3r9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2 — чистим датасет\n",
        "# Пробуем нормализацию и проверку на тип данных, чтобы не терять Й\n",
        "def cleaning(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "    # Нормализуем в NFC (каноническая форма)\n",
        "    text = unicodedata.normalize(\"NFC\", text)\n",
        "    cleaned_text = re.sub(r'[^а-яА-ЯёЁ\\s]+|\\s+', ' ', text).strip()\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', text)\n",
        "    return cleaned_text if cleaned_text else pd.NA"
      ],
      "metadata": {
        "id": "dwzoKzlTVyEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Grammatical Sentences'].apply(lambda x: [cleaning(string) for string in x])"
      ],
      "metadata": {
        "id": "1QPYZjpk1A7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Grammatical Sentences'].dropna().to_list"
      ],
      "metadata": {
        "id": "KNVR7U_P2_AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 — создаем список сникальных слов и дублируем его (основа для разделения на слоги)\n",
        "\n",
        "# Функция для получения уникальных слов\n",
        "def get_unique_words(sents):\n",
        "    # Объединяем все строки столбца в один текст\n",
        "    all_text = ' '.join()\n",
        "\n",
        "    # Преобразуем текст в нижний регистр и токенизируем текст\n",
        "    tokens = word_tokenize(all_text.lower())\n",
        "\n",
        "    # Сохраняем уникальные слова\n",
        "    unique_words = set(tokens)\n",
        "    unique_words_array = np.array(list(unique_words))\n",
        "    return unique_words_array"
      ],
      "metadata": {
        "id": "G5tQ6WqhY-IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Получаем уникальные слова для всего столбца\n",
        "unique_words = get_unique_words(df)\n",
        "\n",
        "# Добавляем массив NumPy как новый столбец в DataFrame\n",
        "# Поскольку список уникальных слов может быть длинным, подгоняем его под размер DataFrame\n",
        "df_unique_words = pd.DataFrame(list(unique_words), columns=['Words'])\n",
        "\n",
        "df_unique_words"
      ],
      "metadata": {
        "id": "6F5sWkosZGOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем второй столбец как копию первого\n",
        "df_unique_words[\"Hyphenated_Words\"] = df_unique_words[\"Words\"]\n",
        "# теперь ко второму столбцу надо применить токенизацию по слогам"
      ],
      "metadata": {
        "id": "MSUOaYM_bnq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 — разделение на слоги\n",
        "def syllables(text):\n",
        "    vowel = 'аеёиоуыэюя'\n",
        "    cons = 'бвгджзлмнрсхцчшщ'\n",
        "    brief = 'кпстф'\n",
        "    voiced = 'й'\n",
        "    deaf = 'ьъ'\n",
        "    other = 'бвгджзйклмнпрсстфхцчшщ'\n",
        "\n",
        "    def _is_not_last_sep(txt):\n",
        "        return any(ch in vowel for ch in txt)\n",
        "\n",
        "    def _add_sep(force_sep):\n",
        "        nonlocal current_syllable, syllables\n",
        "        if force_sep:\n",
        "            current_syllable += ' '\n",
        "            return\n",
        "\n",
        "        if not re.search(f'[{vowel}]', current_syllable):\n",
        "            return\n",
        "\n",
        "        syllables.append(current_syllable)\n",
        "        current_syllable = ''\n",
        "\n",
        "    syllables = []\n",
        "    current_syllable = ''\n",
        "    words = text.split()\n",
        "\n",
        "    for word in words:\n",
        "        if len(word) < 2:\n",
        "            word = word.replace('-', '')\n",
        "\n",
        "        for i, char in enumerate(word):\n",
        "            current_syllable += char\n",
        "            next_char = word[i+1] if i+1 < len(word) else ''\n",
        "\n",
        "            if not next_char or next_char not in 'абвгдеёжзийклмнопрстуфхцчшщыьэюя':\n",
        "                continue\n",
        "\n",
        "            if (i != 0 and i != len(word)-1 and char in voiced and\n",
        "                _is_not_last_sep(word[i+1:])):\n",
        "                _add_sep(False)\n",
        "                continue\n",
        "\n",
        "            if (i < len(word)-1 and char in vowel and\n",
        "                next_char in vowel):\n",
        "                _add_sep(False)\n",
        "                continue\n",
        "\n",
        "            if (i < len(word)-2 and char in vowel and\n",
        "                word[i+1] in other and word[i+2] in vowel):\n",
        "                _add_sep(False)\n",
        "                continue\n",
        "\n",
        "            if (i < len(word)-2 and char in vowel and\n",
        "                word[i+1] in brief and word[i+2] in other and\n",
        "                _is_not_last_sep(word[i+1:])):\n",
        "                _add_sep(False)\n",
        "                continue\n",
        "\n",
        "            if (i > 0 and i < len(word)-1 and char in cons and\n",
        "                word[i-1] in vowel and next_char not in vowel and\n",
        "                next_char not in deaf and\n",
        "                _is_not_last_sep(word[i+1:])):\n",
        "                _add_sep(False)\n",
        "                continue\n",
        "\n",
        "            if (i < len(word)-1 and char in deaf and\n",
        "                (next_char not in vowel or _is_not_last_sep(word[:i]))):\n",
        "                _add_sep(False)\n",
        "                continue\n",
        "\n",
        "        _add_sep(True)\n",
        "\n",
        "    if current_syllable:\n",
        "        syllables.append(current_syllable)\n",
        "\n",
        "    return '-'.join(syllables)"
      ],
      "metadata": {
        "id": "DktZIm5nG2eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Применяем функцию ко всему столбцу \"Words\" в df_unique_words\n",
        "df_unique_words['Hyphenated_Words'] = df_unique_words['Words'].apply(syllables)\n",
        "\n",
        "# Результат\n",
        "df_unique_words['Hyphenated_Words']"
      ],
      "metadata": {
        "id": "dAKfmMN6G8Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #5 – создаем словарь,  где ключи - слова из первого столбца, а значения - из второго, и сохраняем в формате json\n",
        "def save(df_unique_words):\n",
        "    words_dict = dict(zip(df_unique_words[\"Hyphenated_Words\"], df_unique_words[\"Words\"]))\n",
        "\n",
        "    with open(\"hyphenated_words_dict.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "        json.dump(words_dict, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    return words_dict"
      ],
      "metadata": {
        "id": "FEwGP6SXb91L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка результата\n",
        "save(df_unique_words)"
      ],
      "metadata": {
        "id": "RY_560svcxhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Делим на слоги датасет из Викисловаря**"
      ],
      "metadata": {
        "id": "ep1-diP8FtH1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjY69iZiz3YI"
      },
      "outputs": [],
      "source": [
        "!wget data.txt https://raw.githubusercontent.com/vifirsanova/stat-llm/refs/heads/main/morphs_output.txt\n",
        "\n",
        "with open(\"morphs_output.txt\", \"r\", encoding=\"utf-8\") as morphs:\n",
        "    words = morphs.read().strip().split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "id": "Shfmdy9uGvmU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "word_series = pd.Series(words, name = 'words')\n",
        "word_series"
      ],
      "metadata": {
        "id": "5xRyxI7CG2rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаём DataFrame с двумя одинаковыми столбцами\n",
        "df = pd.DataFrame({\n",
        "    'words': word_series,\n",
        "    'hyphenated_words': word_series  # дубликат\n",
        "})\n",
        "df"
      ],
      "metadata": {
        "id": "PYXpp2rBG_Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделение на слоги\n",
        "import re\n",
        "def syllables(text):\n",
        "    vowel = 'аеёиоуыэюя'\n",
        "    cons = 'бвгджзлмнрсхцчшщ'\n",
        "    brief = 'кпстф'\n",
        "    voiced = 'й'\n",
        "    deaf = 'ьъ'\n",
        "    other = 'бвгджзйклмнпрсстфхцчшщ'\n",
        "\n",
        "    def _is_not_last_sep(txt):\n",
        "        return any(ch in vowel for ch in txt)\n",
        "\n",
        "    def _add_sep(force_sep):\n",
        "        nonlocal current_syllable, syllables\n",
        "        if force_sep:\n",
        "            current_syllable += ' '\n",
        "            return\n",
        "\n",
        "        if not re.search(f'[{vowel}]', current_syllable):\n",
        "            return\n",
        "\n",
        "        syllables.append(current_syllable)\n",
        "        current_syllable = ''\n",
        "\n",
        "    syllables = []\n",
        "    current_syllable = ''\n",
        "    words = text.split()\n",
        "\n",
        "    for word in words:\n",
        "        if len(word) < 2:\n",
        "            word = word.replace('-', '')\n",
        "\n",
        "        for i, char in enumerate(word):\n",
        "            current_syllable += char\n",
        "            next_char = word[i+1] if i+1 < len(word) else ''\n",
        "\n",
        "            if not next_char or next_char not in 'абвгдеёжзийклмнопрстуфхцчшщыьэюя':\n",
        "                continue\n",
        "\n",
        "            if (i != 0 and i != len(word)-1 and char in voiced and\n",
        "                _is_not_last_sep(word[i+1:])):\n",
        "                _add_sep(False)\n",
        "                continue\n",
        "\n",
        "            if (i < len(word)-1 and char in vowel and\n",
        "                next_char in vowel):\n",
        "                _add_sep(False)\n",
        "                continue\n",
        "\n",
        "            if (i < len(word)-2 and char in vowel and\n",
        "                word[i+1] in other and word[i+2] in vowel):\n",
        "                _add_sep(False)\n",
        "                continue\n",
        "\n",
        "            if (i < len(word)-2 and char in vowel and\n",
        "                word[i+1] in brief and word[i+2] in other and\n",
        "                _is_not_last_sep(word[i+1:])):\n",
        "                _add_sep(False)\n",
        "                continue\n",
        "\n",
        "            if (i > 0 and i < len(word)-1 and char in cons and\n",
        "                word[i-1] in vowel and next_char not in vowel and\n",
        "                next_char not in deaf and\n",
        "                _is_not_last_sep(word[i+1:])):\n",
        "                _add_sep(False)\n",
        "                continue\n",
        "\n",
        "            if (i < len(word)-1 and char in deaf and\n",
        "                (next_char not in vowel or _is_not_last_sep(word[:i]))):\n",
        "                _add_sep(False)\n",
        "                continue\n",
        "\n",
        "        _add_sep(True)\n",
        "\n",
        "    if current_syllable:\n",
        "        syllables.append(current_syllable)\n",
        "\n",
        "    return '-'.join(syllables)"
      ],
      "metadata": {
        "id": "-HHbqtpuHmfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['hyphenated_words'] = df['hyphenated_words'].apply(lambda x: syllables(x))\n",
        "df"
      ],
      "metadata": {
        "id": "NLR9R8yGIYmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/content/syllables.csv', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "6-taKAEjIdm7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}